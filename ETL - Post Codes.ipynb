{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2b54af5b-15e4-4403-9880-3e9aefc04785",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### En este notebook encontrará los siguientes puntos:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d78db616-acce-45f6-9f29-7b0b53e3b55c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Etapas del ETL:\n",
    "\n",
    "1. Recepción de archivos CSV (AWS Lambda + AWS S3 -or- Databricks Job + AWS S3)\n",
    "2. Carga de archivos CSV (Spark)\n",
    "3. Manejo de errores y filtrado de registros incompletos (Spark + AWS SES)\n",
    "4. Consulta de la API postcodes.io (Python)\n",
    "5. Almacenamiento de códigos postales (MongoDB)\n",
    "6. Respaldo de archivos CSV en Data Lake (AWS S3)\n",
    "\n",
    "Adicional:\n",
    "\n",
    "- Pruebas unitarias\n",
    "- Control de Versiones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d7eeae80-ba44-4c8e-8e06-7e3f5d090c62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "import smtplib\n",
    "import requests\n",
    "import urllib.parse\n",
    "from math import ceil\n",
    "from pathlib import Path\n",
    "from datetime import date\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from email.mime.text import MIMEText\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql import functions as F\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from pyspark.sql.functions import lit, row_number, col"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "255ab4de-ac80-46f6-9088-63ba376535f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 1. Recepción de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ee95dcc4-fa00-4a80-a7e3-3d1652d44c12",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para la recepción de archivos se me ocurren dos soluciones distintas, las cuales comentaré a continuación:\n",
    "\n",
    "**Opción 1 (AWS S3 + AWS Lambda):**\n",
    "\n",
    "Esta opción es la indicada si requieren cargarse archivos de forma continua al sistema. La idea consiste en, configurar un trigger que ejecute la función lambda, cada vez que se suba un CSV a un bucket S3.\n",
    "\n",
    "Las funciones lambda escalan automáticamente recursos, por lo que, la disponibilidad del sistema no se vería comprometida ante un aumento en el número de archivos a procesar. Además, tanto las funciones lambda como los buckets S3 son recursos de muy bajo costo monetario.\n",
    "\n",
    "**Opción 2 (AWS S3 + Job Databricks):**\n",
    "\n",
    "Esta sería la opción indicada si requieren cargarse archivos con muchos registros (mayores al millón), y si la carga continua de datos al warehouse no es una necesidad.\n",
    "\n",
    "Databricks ejecuta Spark de forma nativa, además de poseer otras características que lo vuelven una gran herramienta para procesamientos de grandes volúmenes de datos. Sin embargo, si se requiriera una solución que esté ejecutándose de forma continua, sería necesario tener un cluster específico para dicha solución, lo cual elevaría los constos. \n",
    "\n",
    "Otro detalle a tomar en cuenta, es que no vale la pena contratar Databricks para la ejecución de un sólo servicio. Databricks es una plataforma para la integración de todos los procesos de un área de Analítica. Sin embargo, otra gran ventaja es que, si ya se cuenta con la herramienta, no hace falta realizar configuraciones adicionales, a diferencia de la opción 1 con la función lambda.\n",
    "\n",
    "----------------------------------------------------------------------------------------------\n",
    "\n",
    "Para este ejercicio, utilizaré Databricks por comodidad, sin embargo, para elegir la infraestructura adecuada, es necesario un análisis detallado de las necesidades de un proyecto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cd275973-ca9d-419f-b5aa-6194c3d047e0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "A continuación se presentan dos diagramas, correspondientes a las opciones de recepción de archivos explicadas arriba."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b8695067-dcb3-4f30-8812-0c435fa8e95e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opción 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f71a489-7aaa-4c65-a310-3b6aad78dc7e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"./Imagenes/Arquitectura/opcion1.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Se sube un CSV al bucket de S3.\n",
    "2. Se “dispara” una función lambda, la cuál realiza limpieza sobre los registros del CSV, además de nutrir la data con información adicional de postcodes.io.\n",
    "3. Se respalda la data cruda (CSVs) en un bucket distinto de S3.\n",
    "4. Se carga la información corregida y aumentada al Warehouse en MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a92cd549-fd53-4269-bb2d-02c6e472372f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Opción 2:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1d328c9-db7a-4548-8dc1-739066b5db1d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<img src=\"./Imagenes/Arquitectura/opcion2.png\" width=\"500\" height=\"500\">\n",
    "\n",
    "<br>\n",
    "\n",
    "1. Databricks consulta el bucket de S3 al que se suben los CSVs crudos.\n",
    "2. Se activa una función lambda, la cuál realiza limpieza sobre los registros del CSV, además de nutrir la data con información adicional de postcodes.io.\n",
    "3. Se respalda la data cruda (CSVs) en un bucket distinto de S3.\n",
    "4. Se carga la información corregida y aumentada al Warehouse en MongoDB."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "245680ca-7266-4ca4-a5b7-babeb5bd513f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nota 1:** Ya que esta etapa de la solución no requiere escribir código, y está más relacionada con el levantamiento de infraestructura, más que adjuntar código redacto una breve explicación respecto a las herramientas seleccionadas.\n",
    "\n",
    "**Nota 2:** Ya que pueden surgir problemas al tratar de desplegar imágenes en los notebook, también anexo un PDF al repositorio en GitHub, en el que muestro los diagramas de las 2 arquitecturas propuestas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d92b30c-4b74-4118-b4f7-25d1ec64a7e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 2. Carga de archivos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aac16c69-8eee-403e-ad3e-8f78bda617de",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para la carga de datos utilizaría Spark, ya que el archivo de prueba tiene más de 2M de registros, así que asumíré que se estarán cargando grandes volúmenes de datos al sistema.\n",
    "\n",
    "- **Nota 1:** Para este ejercicio me encuentro utilizando la plataforma Databricks, la cual ejecuta Spark de de forma nativa, por esa razón, puedo utilizar el comando spark.read() sin necesidad de crear una nueva sesión de Spark.\n",
    "\n",
    "- **Nota 2:** Databricks se encuentra asociado a una cuenta AWS, en la cual se encuentra el bucket al que estoy cargando los CSVs. Debido a esto, puedo hacer la lectura de los archivos simplemente indicando la dirección de dicho bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6ef1e7d7-94f1-4f07-849e-f0ba2d72d4d1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "#                         Función para el envío de correos                              #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "# -- Llaves de acceso a SES\n",
    "\n",
    "USERNAME_SMTP = '...'\n",
    "PASSWORD_SMTP = '...'\n",
    "host = '...'\n",
    "port = '...'\n",
    "\n",
    "# -- Variables genéricas opara el envío de los correos de error\n",
    "\n",
    "SENDERNAME = \"...\"\n",
    "RECIPIENT = \"...\"\n",
    "COPY= \"\"\n",
    "SUBJECT = \"Error en CSV\"\n",
    "\n",
    "# -- Función para el envío de correos\n",
    "\n",
    "def send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML):\n",
    "\n",
    "    # Create message container - the correct MIME type is multipart/alternative.\n",
    "    msg = MIMEMultipart('alternative')\n",
    "    msg['Subject'] = SUBJECT\n",
    "    msg['From'] = SENDERNAME\n",
    "    msg['To'] = RECIPIENT\n",
    "    msg[\"Cc\"] = COPY\n",
    "\n",
    "    # Record the MIME types of both parts - text/plain and text/html.\n",
    "    text = MIMEText(BODY_HTML, 'html')\n",
    "\n",
    "    # Attach parts into message container.\n",
    "    # According to RFC 2046, the last part of a multipart message, in this case\n",
    "    # the HTML message, is best and preferred.\n",
    "    msg.attach(text)\n",
    " \n",
    "    server = smtplib.SMTP(host, port)\n",
    "    server.ehlo()\n",
    "    server.starttls()\n",
    "    #stmplib docs recommend calling ehlo() before & after starttls()\n",
    "    server.ehlo()\n",
    "    server.login(USERNAME_SMTP, PASSWORD_SMTP)\n",
    "    server.sendmail(msg[\"From\"], msg[\"To\"].split(\",\") + msg[\"Cc\"].split(\",\"), msg.as_string())\n",
    "    #https://www.tfzx.net/article/20923.html\n",
    "    server.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5e0888e1-3ab3-497a-aa3d-58236863de20",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "#                Se cargan los CSVs en AWS S3 en un Dataframe de Spark                  #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"post_codes\").getOrCreate()\n",
    "\n",
    "try:\n",
    "    df = spark.read.format('csv').options(header='true', inferSchema='true').load('s3:/.../pruebas_locas/*.csv')\n",
    "except:\n",
    "    BODY_HTML = \"\"\"Ocurrió un error en la ejecución del ETL, esto puede deberse a los siguientes problemas:\n",
    "                 <br>     - No se han subido nuevos archivos al sistema.\n",
    "                 <br>     - Los archivos subidos al sistema están corruptos.\n",
    "                 <br>     - Los archivos subidos al sistema no son archivos csv.\n",
    "                 <br>Fecha: {}\"\"\".format(date.today().strftime('%Y-%m-%d'))\n",
    "    send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)\n",
    "    dbutils.notebook.exit(\"Ocurrió un error en la ejecución del ETL.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c1d70a0-3c84-4edf-9768-bb60bb054d3a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 3. Manejo de errores y filtrado de registros incompletos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af118495-cdb7-43cf-83e8-38ee79eb2586",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Se manejan 3 tipos de errores distintos: \n",
    "\n",
    "  - CSVs vacíos\n",
    "  - CSVs con columnas incorrectas\n",
    "  - CSV con valores no numéricos\n",
    "  \n",
    "Cualquiera de esos errores desencadena la finalización del job, y la notificación del archivo \"defectuoso\" a los correos seleccionados.\n",
    "\n",
    "Para esta etapa, opté por enviar una notificación por correo, ya que he notado que resulta más efectivo notificar de errores de esta forma. Para esta notificación, utilicé el Simple Email Service (SES) de AWS. En caso de utilizar lambdas en vez de Databricks, podría utilizarse el Simple Notification Service (SNS), lo cual facilitaría el envío a múltiples destinatarios.\n",
    "\n",
    "**Nota:** Tuve que definir la función de envío de correos en el paso anterior, ya que puede haber errores en la carga de CSVs con Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f1cfe64-1ba3-4763-b3ed-f71eff9779ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "# Si no se han cargado nuevos archivos al bucket (o si el CSV está vacío), el job se    #\n",
    "# detiene para evitar errores                                                           #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "if (df.rdd.isEmpty()):\n",
    "    BODY_HTML = \"\"\"Los archivos subidos al sistema se encuentran vacíos\n",
    "                 <br>Fecha: {}\"\"\".format(date.today().strftime('%Y-%m-%d'))\n",
    "    send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)\n",
    "    dbutils.notebook.exit(\"Los archivos subidos al sistema se encuentran vacíos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "925aa578-8c6f-47c4-9a66-9e3177204c52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "#   Si se han agregado CSVs con columnas distintas a las esperadas, el job se detiene   #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "if (not all(elem in ['lat', 'lon'] for elem in df.columns)):\n",
    "    BODY_HTML = \"\"\"Los archivos subidos al sistema tienen columnas incorrectas\n",
    "                 <br>Fecha: {}\"\"\".format(date.today().strftime('%Y-%m-%d'))\n",
    "    send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)\n",
    "    dbutils.notebook.exit(\"Los archivos subidos al sistema tienen columnas incorrectas\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "098a5ebe-aa1a-45db-9205-0a175103e96b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "#        Si el CSV contiene valores no numéricos (o vacíos), el job se detiene          #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "tmp = df.select(\n",
    "  F.col(\"lat\").cast(\"int\").isNotNull().alias(\"lat\"),\n",
    "  F.col(\"lon\").cast(\"int\").isNotNull().alias(\"lon\")\n",
    ")\n",
    "\n",
    "if(tmp.filter(tmp.lat == False).count() > 0 or tmp.filter(tmp.lon == False).count() > 0):\n",
    "    BODY_HTML = \"\"\"Los archivos subidos al sistema contienen registros con valores no numéricos (o vacíos)\n",
    "                 <br>Fecha: {}\"\"\".format(date.today().strftime('%Y-%m-%d'))\n",
    "    send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)\n",
    "    dbutils.notebook.exit(\"Los archivos subidos al sistema contienen registros con valores no numéricos (o vacíos)\")\n",
    "\n",
    "# https://www.py4u.net/discuss/192652"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e8bb5e93-8708-49ba-bb3d-3cb627ab7039",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nota:** El manejo de errores podría realizarse de una forma mucho más sofisticada, indicando la línea exacta del csv en la que se presenta el error. Por cuestiones de tiempo no implementaré dicha estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "db486824-f557-40ed-ba06-ed7a8ede71d7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 4. Consulta de la API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d6ae597-8da2-4a7c-8255-f91c7b0b6739",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Ya que las llamadas a la API son limitadas, estas se haran por \"batches\" de coordenadas. El número de llamadas está definido en el código en la variable \"llamadas_api\". Desconozco el límite de solicitudes que se pueden realizar a la API, así que, para este ejercicio, asigné dicho número de forma arbitraria.\n",
    "\n",
    "**Nota:** Tras estar jugando con la API, descubrí que esta tiene un límite de 100 coordenadas por petición, debido a esto, las pruebas realizadas a partir de aquí conllevan batches no mayores a ese número."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "11697099-469c-40ea-b624-1b967b997ec5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "# Se renombran las columnas \"lat\" y \"lon\" de acuerdo a los requerimientos de la API      #\n",
    "# (postcodes.ip), además, se agrega una columna \"limit\", para que las peticiones a la    #\n",
    "# API sólo devuelvan 1 resultado por coordenada geográfica.                              #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "df = df.withColumnRenamed(\"lat\",\"latitude\") \\\n",
    "        .withColumnRenamed(\"lon\",\"longitude\") \\\n",
    "        .withColumn('limit', lit(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0f2645e8-3d1b-479f-8841-180bee850787",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "# Se agrega un \"índice\" al dataframe de Spark, para poder tomar \"batches\" de coordenadas #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "w = Window().partitionBy(lit('a')).orderBy(lit('a'))\n",
    "df_indexed = df.withColumn(\"index\", row_number().over(w))\n",
    "\n",
    "# Para poder seleccionar un rango de filas del dataframe, se requiere crear una columna que funja de indice: \n",
    "# https://stackoverflow.com/questions/55690705/how-to-select-a-range-of-rows-from-a-dataframe-in-pyspark\n",
    "# https://stackoverflow.com/questions/53042432/creating-a-row-number-of-each-row-in-pyspark-dataframe-using-row-number-functi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2a15384f-2dad-4270-b285-1d6c08f2f1e6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "#  Se calcula el número de pares de coordenadas que tendrá cada batch en las peticiones  #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "llamadas_api = 5\n",
    "batch_size = ceil(df.count()/llamadas_api)\n",
    "\n",
    "if (batch_size > 100):\n",
    "    BODY_HTML = \"\"\"Los batches no pueden ser mayores a 100 coordenadas, debido a restricciones de la API\n",
    "                 <br>Fecha: {}\"\"\".format(date.today().strftime('%Y-%m-%d'))\n",
    "    send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)\n",
    "    dbutils.notebook.exit(\"Los batches no pueden ser mayores a 100 coordenadas, debido a restricciones de la API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "25698c00-494a-48c5-ae3d-1aac0f3a765d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "# Se van obteniendo los batches de coordenadas mediante un ciclo for() y, a la vez, se   #\n",
    "# castean a formato JSON, para poder realizar peticiones de batches a la API.            #\n",
    "#                                                                                        #\n",
    "# Adicional, se agrega cada batch como elemento de la key \"geolocations\", que es el      #\n",
    "# campo que utiliza la API para recibir batches de coordenadas.                          #\n",
    "#                                                                                        #\n",
    "# Al final de esta celda, se obtiene un arreglo de JSONS: [{\"geolocations\" : [batch_1]}, #\n",
    "#                                                          {\"geolocations\" : [batch_2]}, #\n",
    "#                                                          .                             #\n",
    "#                                                          .                             #\n",
    "#                                                          .                             #\n",
    "#                                                          {\"geolocations\" : [batch_n]}] #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "batches = []\n",
    "\n",
    "for i in range(llamadas_api):\n",
    "    down_index = batch_size*i+1\n",
    "    up_index = batch_size*(i+1)\n",
    "    batches = batches + [{\"geolocations\": [json.loads(row) for row in df_indexed.filter(col(\"index\") \\\n",
    "                                                                                .between(down_index, up_index)) \\\n",
    "                                                                                .select(\"latitude\", \"longitude\", \"limit\") \\\n",
    "                                                                                .toJSON().collect()]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d35b973a-6f12-40a2-98ac-cd80f790dbbf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "#                 Se manda llamar a la API por cada batch de coordenadas                 #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "responses = []\n",
    "\n",
    "for batch in batches:\n",
    "    headers = {'Content-type': 'application/json', 'Accept': 'text/plain'}\n",
    "    response = requests.post('https://api.postcodes.io/postcodes/', json=batch, headers=headers).json()\n",
    "    \n",
    "    if (response['status'] != 200):\n",
    "        BODY_HTML = \"\"\"Ocurrió un error desconocido al tratar de envíar un batch de coordenadas a la API.\n",
    "                   Favor de contactar con el desarrollador.\n",
    "                   <br>Fecha: {}\"\"\".format(date.today().strftime('%Y-%m-%d'))\n",
    "        send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)\n",
    "        dbutils.notebook.exit(\"Ocurrió un error desconocido al tratar de envíar un batch de coordenadas a la API.\")\n",
    "        \n",
    "        responses = responses + response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2055fcf0-9522-4080-a487-68c532feae16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "# Se recorre la lista con todas las respuestas devueltas por la API, si el resultado es  #\n",
    "# nulo (null), las coordenadas se agrega a una lista que serán reportadas por correo, en #\n",
    "# caso contrario, el resultado se agrega a la lista de respuestas que serán agregadas a  #\n",
    "# la BD.                                                                                 #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "to_report = []\n",
    "to_bd = []\n",
    "\n",
    "for response in responses:\n",
    "    if(response['result'] == None):\n",
    "        to_report = to_report + [[response['query']['latitude'], response['query']['longitude']]]\n",
    "    else:\n",
    "        to_bd = to_bd + response['result']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "702ce5ba-d171-46b2-aec6-a2111cd41b9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------------------- #\n",
    "# Los pares (latitude, longitude) sin un código postal asociado son enviadas por correo. #\n",
    "# -------------------------------------------------------------------------------------- #\n",
    "\n",
    "to_report = \"<br>\".join(', '.join(str(y) for y in x) for x in to_report)\n",
    "\n",
    "BODY_HTML = \"\"\"Se han subido coordenadas (latitude, longitude) que no empatan con ningún código postal.\n",
    "               <br>\n",
    "               <br>A continuación la lista:\n",
    "               <br>\n",
    "               <br>{1}\n",
    "               <br>\n",
    "               <br>\n",
    "               <br>Fecha: {0}\"\"\".format(date.today().strftime('%Y-%m-%d'), to_report)\n",
    "send(SENDERNAME, RECIPIENT, COPY, SUBJECT, BODY_HTML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a6afe88-1d2e-45e3-a581-5ac7209b4acd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nota:** Se podría desarrollar un proceso más sofisticado, el cual identifique las coordenadas que no empataron con un código postal y, posteriormente, indique dichas coordenadas al usuario final. Sin embargo, por cuestiones de tiempo no implementaré dicha lógica."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "82c3ae3d-63e1-4082-b367-fa7014260097",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 5. Almacenamiento de códigos postales en BD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b0a71aa-9946-49e8-84da-2c28448bbba8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "El resultado devuelto por la API poscodes.io es un arreglo de JSONs, debido a esto, dicha respuesta puede ser almacenada en MongoDB sin necesidad de realizar procesamiento adicional.\n",
    "\n",
    "Por comodidad, elegí MongoDB para el almacenamiento de CPs, sin embargo, dependiendo del uso que se le fuera a dar a la data, podría optar por una base optimizada para análisis (p.e. Redshift)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "04523b65-8c9b-4bae-b738-cdfb13c2265d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user = '...'\n",
    "password = '...'\n",
    "db = '...'\n",
    "host = '...'\n",
    "\n",
    "mongo_uri = \"mongodb+srv://\" + urllib.parse.quote_plus(user) + \":\" + password + \"@\" + host + \"/\" + db + \"?retryWrites=true&w=majority&ssl=true&ssl_cert_reqs=CERT_NONE\"\n",
    "\n",
    "client = MongoClient(mongo_uri)\n",
    "mydb = client[db]\n",
    "\n",
    "# Se obtiene la colección\n",
    "\n",
    "mycol = mydb['postal_codes']\n",
    "\n",
    "# Se inserta la data \n",
    "\n",
    "mycol.insert_many(to_bd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0cd5e14c-49c4-4c8d-b0ca-d86404ef7a48",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nota:** Haría falta una validación, para verificar que los pares \"longitud, latitud\" no hayan sido previamente ingresados a la BD, sin emabrgo, por razones de tiempo y que no es un requerimiento de este ejercicio, no realizaré dicha validación."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b390df4-1d7c-4ae3-8730-35270af67dbb",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### 6. Respaldo de archivos en Data Lake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0c69e13b-e7ef-4cab-8988-33a8bf4eb670",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para conservar el archivo original (.csv), opté por crear un nuevo directorio en el bucket S3. Además, agrego la fecha al nombre del archivo, para poder diferenciar archivos en caso de que se suban dos o más con el mismo nombre.\n",
    "\n",
    "**Nota 1:** Esta parte del ETL podría realizarse sin asumir un role mediante el cliente STS. Tuve que realizarlo de esta forma debido a configuración de mi usuario de AWS.\n",
    "\n",
    "**Nota 2:** Esta etapa de la ejecución debe dejarse al final del ETL, esto debido a la \"lazy evaluation\" de Spark. Spark no procesa la data hasta que se le es indicado mediante instrucciones, por consiguiente, si la data cruda es borrada o actualizada antes de enviar dicha orden a Spark, se generará un error en la ejecución."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "686964e3-f359-4cc9-8a44-02355f0c93c1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------------------- #\n",
    "#        Se asume un role iam para poder utilizar ciertas funcionalidades de S3         #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "rol_s3 = \"arn:aws:iam::...:role/databricks-s3\"\n",
    "\n",
    "sts_client = boto3.client('sts')\n",
    "\n",
    "assumed_role_object = sts_client.assume_role (RoleArn = rol_s3,\n",
    "                                              RoleSessionName = \"AssumeRoleSession1\")\n",
    "    \n",
    "credentials = assumed_role_object['Credentials']\n",
    "s3_resource = boto3.resource('s3',\n",
    "                            aws_access_key_id=credentials['AccessKeyId'],\n",
    "                            aws_secret_access_key= credentials ['SecretAccessKey'],\n",
    "                            aws_session_token = credentials['SessionToken'],)\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "#                              Se accede al bucket deseado                              #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "bucket = \"...\"\n",
    "\n",
    "my_bucket = s3_resource.Bucket(bucket)\n",
    "\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "# Se copian todos los CSV (archivos crudos) a otra dirección en bucket S3, y se         #\n",
    "# eliminan del bucket actual.                                                           #\n",
    "# ------------------------------------------------------------------------------------- #\n",
    "\n",
    "fecha = date.today().strftime('%Y-%m-%d')\n",
    "\n",
    "for obj in my_bucket.objects.filter(Delimiter='/', Prefix='pruebas_locas/'):\n",
    "    if obj.key.endswith('.csv'):\n",
    "        copy_source = {'Bucket': bucket,\n",
    "                   'Key': obj.key}\n",
    "        new_file = obj.key.split('/')[0] + \"/data_lake/\" + obj.key.split('/')[1].split('.')[0] + \"_\" + fecha + '.csv'\n",
    "        s3_resource.meta.client.copy(copy_source, bucket, new_file)\n",
    "        s3_resource.Object(bucket, obj.key).delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d043b863-78da-4ed8-a2ef-f63729b84a25",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Pruebas unitarias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e17be680-a9bd-49d2-a451-98ebed37b82f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Para este ejercicio, más que realizar pruebas unitarias, realicé pruebas de \"semi-integración\"; llamaré a las pruebas de esta forma ya que, para validar el funcionamiento de alguna sección de la etapa 3 del ETL (p.e. la condición que valida que los csv sólo contengan valores numéricos), deben ejecutarse primero las etapas 1 y 2, para validar alguna sección de la etapa 4, deben ejecutarse primero las etapas 1, 2 y 3, y así sucesivamente.\n",
    "\n",
    "Realicé pruebas de semi-integración porque las considero mucho más \"ágiles\" que las tradicionales pruebas unitarias. Además, son un tipo de pruebas que se vuelven fáciles de realizar gracias a los notebooks de código, a diferencia de los tradicionales entornos de desarrollo que no pueden ejecutar secciones de código de manera independiente.\n",
    "\n",
    "A continuación, se enlistan las pruebas realizadas en cada etapa del ETL,\n",
    "\n",
    "**Etapa 1:**\n",
    "\n",
    "N/A\n",
    "\n",
    "**Etapa 2:**\n",
    "- Ejecución del ETL sin haber subido CSVs a AWS (revisar /Imagenes/Pruebas/prueba1.png)\n",
    "- Ejecución del ETL tras haber subido TXTs a AWS (revisar /Imagenes/Pruebas/prueba2.png)\n",
    "\n",
    "**Etapa 3:**\n",
    "\n",
    "- Carga de 4 CSVs en un dataframe de Spark (revisar /Imagenes/Pruebas/prueba0.png)\n",
    "- Carga de un CSV vacío (revisar /Imagenes/Pruebas/prueba3.png)\n",
    "- Carga de un CSV sin encabezados (revisar /Imagenes/Pruebas/prueba4.png)\n",
    "- Carga de un CSV con encabezados incorrectos (revisar /Imagenes/Pruebas/prueba5.png)\n",
    "- Carga de un CSV con un registro incompleto (sin longitud o latitud) (revisar /Imagenes/Pruebas/prueba6.png)\n",
    "\n",
    "**Etapa 4:**\n",
    "\n",
    "- Asignar la variable \"llamadas_api\" de tal forma que el tamaño de los batches sea mayor a 100 (revisar /Imagenes/Pruebas/prueba7.png)\n",
    "- Cargar de coordenadas \"no asociables\" a un código postal (revisar /Imagenes/Pruebas/prueba8.png)\n",
    "\n",
    "**Etapa 5:**\n",
    "\n",
    "No pude pensar en alguna prueba para esta etapa, sólo verifiqué la correcta carga de los registros a la BD (dicha carga se puede corroborar en la imagen anexa \"prueba9.png\").\n",
    "\n",
    "**Etapa 6:**\n",
    "\n",
    "- Carga de archivos no .csv al bucket S3 de AWS (revisar /Imagenes/Pruebas/prueba10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1cf313a1-fffd-4433-99dd-cc2d2a0d648b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nota:** Además de las pruebas mencionadas arriba, se estuvo validando el funcionamiento correcto de cada celda del notebook durante todo el desarrollo del ejercicio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5d2f1fca-8738-4312-a105-f8a402f13972",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Control de Versiones"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "935ab412-42b8-4db5-8dd0-2d5f20a3b6d6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Debido a que la plataforma que utilicé (Databricks) cuenta con su propio Control de Versiones, fue que no utilicé algún repositorio tipo GitLab o GitHub para llevar dicho control, sin embargo, conozco perfectamente dichas herramientas, y no tengo problema alguno con los conceptos e instrucciones de Push y Pull."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dc924992-9f22-459c-b0b3-a87a5379e2e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "**Nota al pie 1:** Por cuestiones de privacidad, no comparto las llaves para acceder a mis cuentas de AWS ni de MongoDB, así como nombres de buckets S3. Para poder ejecutar este notebook, hace falta agregar llaves de una cuenta propia, así como el nombre de algún bucket relacionado a dicha cuenta.\n",
    "\n",
    "**Nota al pie 2:** Lo mismo aplica para las variables definidas en la celda 12 para el envío de correos (SENDERNAME, RECIPIENT, COPY...). Además, tendría que darse de alta en AWS la dirección desde la que se envían los correos.\n",
    "\n",
    "**Nota al pie 3:** En caso de no utilizarse Databricks, sería necesario descomentar una línea en la celda 10 de este notebook, la cuál crea una sesión de Spark. Además, deberían eliminarse todas las sentencias que hacen uso de la librería \"dbutils\"."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "ETL - Post Codes",
   "notebookOrigID": 2654311738143125,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
